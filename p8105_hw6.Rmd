---
title: "P8105 Homework 6"
author: "Jesse R. Ames"
date: "12/4/2021"
output: github_document
---

## Problem 1

### Load and clean the data

```{r loaddata}
library(tidyverse)
birthweight <- read_csv("data/birthweight.csv")
skimr::skim(birthweight)
```

No missing data, but we might want to convert some `numeric` columns to `factor`

```{r cleandata}
bw <- birthweight %>%
  mutate(
    babysex = as.character(babysex),
    babysex = fct_recode(babysex, Male = "1", Female = "2"),
    
    frace = as.character(frace),
    frace = fct_recode(frace, White = "1", Black = "2", Asian = "3",
                            "Puerto Rican" = "4", Other = "8", Unknown = "9"),
    
    mrace = as.character(mrace),
    mrace = fct_recode(mrace, White = "1", Black = "2", Asian = "3",
                            "Puerto Rican" = "4", Other = "8"),
    
    malform = as.logical(malform)
  )
```

### Multiple linear regression

We seem to have a lot of variables here, and a lot of possible interactions. We want to create a model that is both parsimonious and explanatorily powerful. Accordingly, I will choose my model using BIC, starting from the full model (all linear terms, no interactions) and working backward. I will then compare this model's predictive power against two suggested models:

* Model A, with just length at birth and gestational age
* Model B, with head circumference, length, sex, and all interactions between these three

```{r regression}
big_model <- lm(bwt~., data = bw)
step_bic <- step(big_model, trace = 0, k = log(nobs(big_model)), direction = "backward")
summary(step_bic)
```

This model includes the baby's sex, head circumference, and length, and the mother's weight at delivery, weight pre-pregnancy, height, race, and cigarette consumption during pregnancy. Do we violate any assumptions?

```{r diagplots}
library(patchwork)
bw_fit <- bw %>%
  modelr::add_residuals(step_bic) %>%
  modelr::add_predictions(step_bic)

pred_resid <- bw_fit %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")

head_resid <- bw_fit %>%
  ggplot(aes(x = bhead, y = resid)) + geom_point() + labs(x = "Baby's head circumference (cm)",
                                                          y = "Residual")
len_resid <- bw_fit %>%
  ggplot(aes(x = blength, y = resid)) + geom_point() + labs(x = "Baby's length (cm)", y = "Residual")

race_resid <- bw_fit %>%
  ggplot(aes(x = mrace, y = resid)) + geom_violin() + labs(x = "Mother's race", y = "Residual")

(pred_resid + head_resid)/(len_resid + race_resid)
```

It seems like we have a bit of curvature in our residuals. Try raising the power of `bhead` and `blength`

```{r diag1}
fit2 <- lm(bwt ~ babysex + I(bhead^2) + I(blength^2) + delwt + gaweeks +
             mheight + mrace + ppwt + smoken, data = bw)
summary(fit2)
bw_fit2 <- bw %>%
  modelr::add_residuals(fit2) %>%
  modelr::add_predictions(fit2)

bw_fit2 %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

Slightly better. What about normality? Leverage?

```{r diag2}
(plot(fit2, which = 2) + plot(fit2, which = 5))
```

Normality assumption holds. There are a few influential-looking points, but none with a Cook's D of more than 0.5. I will use this as my model going forward. 

### Cross-validation

Now we compare the models in terms of their average root mean square error (RMSE) on 100 random 80-20 splits of the data (80% training, 20% testing).

```{r crossvalidate}
library(modelr)
set.seed(15)
cv_df <- crossv_mc(bw, 100)
    
cv_df <- cv_df %>% 
  mutate(
    model_a  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_b  = map(train, ~lm(bwt ~ (bhead + blength + babysex)^2 + bhead*blength*babysex,
                              data = .x )),
    my_model = map(train, ~lm(bwt ~ babysex + I(bhead^2) + I(blength^2) + delwt + gaweeks +
             mheight + mrace + ppwt + smoken, data = .x))) %>% 
  mutate(
    rmse_model_a = map2_dbl(model_a, test, ~rmse(model = .x, data = .y)),
    rmse_model_b = map2_dbl(model_b, test, ~rmse(model = .x, data = .y)),
    rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

It looks like my model performs the best on average of these three models, though Model B was relatively close.

## Problem 2

### Download the data

```{r download}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

### Boostrap estimates

```{r}
#Bootstrapping is named for the fictional Baron von Munchausen,
#who pulled himself up by his bootstraps when his horse got stuck in the mud

set.seed(15)
munchausen <- weather_df %>%
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy),
    summary = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>%
  pivot_wider(names_from = term, values_from = estimate:p.value) %>%
  mutate(
    log_prod = log(`estimate_(Intercept)` * estimate_tmin)
  ) %>%
  unnest(summary)
  
#Plot distributions
munchausen %>%
  ggplot(aes(x = r.squared)) + geom_density() +
    labs(x = "Estimate r-squared", title = "Distribution of estimated r-squared")

munchausen %>%
  ggplot(aes(x = log_prod)) + geom_density() +
  labs(x = "log(b_0 * b_1)", title = "Distribution of estimated log(b_0 * b_1)")

#Confidence interval - r-squared
lower <- munchausen %>% pull(r.squared) %>% quantile(0.025)
upper <- munchausen %>% pull(r.squared) %>% quantile(0.975)
c(lower, upper)


#Confidence interval - log(B_0 * B_1)
lower2 <- munchausen %>% pull(log_prod) %>% quantile(0.025)
upper2 <- munchausen %>% pull(log_prod) %>% quantile(0.975)
c(lower2, upper2)
```

The distributions of our bootstrap estimates for $\hat{r}^2$ and $\log(\hat{\beta_0}*\hat{\beta_1})$ are each roughly bell-shaped and unimodal, with a slightly steeper bell shape than a normal distribution. The distribution of $\hat{r}^2$ is centered at `r round(munchausen %>% pull(r.squared) %>% median(), 3)` and the distribution of $\log(\hat{\beta_0}*\hat{\beta_1})$ is centered at `r round(munchausen %>% pull(log_prod) %>% median(), 3)`.

